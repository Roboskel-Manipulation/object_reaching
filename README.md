# object_reaching

## Description
This package provides the functionality for an object reaching game. Two objects are placed in front of a human participant and two other objects in front of a robotic manipulator. The human tries to approach one of the objects and through a prediction module we predict the object the human tries to reach. Based on the prediction, the robotic manipulator hits one of its objects.

## Human monitoring
An RGB-D camera is used for the human monitoring and [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is utilized for the 2D human localization. The wrist pixels of the right hand are used for predicting the object the human tries to approach.

## Human motion preprocessing
The detection of the movement is based on the standard deviation of the x and y pixels. Using a sliding window of length N, if the L2 norm of the standard deviation of x and y pixels is greater than a predefined threshold, then we assume that the motion has started. In the meantime, outliers are removed. Outliers are considered NaN values and values which correspond to erroneous measurements. Erroneous pixels are supposed to be pixels whose distance from the last valid pixels is greater than a predetermined threshold. In our experiments, we set the length of the sliding window equal to 10 pixels and the threshold equal to 1.5. The same procedure is followed for detecting the end of the movement with the difference that the length of the sliding window was set to 5 pixels and the threshold equal to 10. These values were chosed experimentally.

## Prediction
Pretrained models are used for predicting the object the human tries to reach. Three kind of models have been tested:
* Feature vector of increasing length containing available pixels at at time t
	* t=t0 ⇒ [x0, y0], t=t1 ⇒ [x0, y0, x1, y1], …

* Feature vector of static length containing the euclidean distances of the most recent pixels to the two objects
	* t=t0 ⇒ [disR0, disL0], t=t1 ⇒ [disR1, disL1], …

* Feature vector of increasing length containing the euclidean distances of the most recent pixels to the two objects
	* t=t0 ⇒ [disR0, disL0], t=t1 ⇒ [disR0, disL0, disR1, disL1], …

The user needs to specify the absolute path of the models.

## Robot motion
The robot moves to a direction according to the output of the human motion prediction module. The user needs to specify the desired velocity with which the robot will hit the objects.

## Files
* `scripts/input_process.py`: Movement detection onset and end and movement filtering
* `scripts/prediction.py`: Prediction of human movement direction
* `scripts/result.py`: Checks who reached the object first (human or robot)
* `src/robot_motion.cpp`: Robot motion generation
* `config/prediction.yaml`: Set the positions (pixels) of the human objects
* `config/object_reaching.yaml`: Set the positions (3D coordinates expressed in the base_link frame) of the robot objects, the initial position of the robot, the velocity with which the robot will hit the objects and the gain of the controller used for regulating the robot commanded velocities.
 
## Run
Run `roslaunch object_reaching object_reaching.launch` to launch the [OpenPose ROS node](https://github.com/firephinx/openpose_ros) used for the human monitoring, the motion detection node, the prediction node and the robot motion node.

Once the nodes have been launched, in another terminal run `rosservice call /next_motion`. This service call will initialize the loop of the game, namely the motion detection node will listen to the 2D output of the OpenPose node. At the end of the game, namely when the robot hits an object, the robot will stay still for 5 seconds and then automatically return to its initial position. Then, run again `rosservice call /next_motion` to start the second experiment. 

## Arguments
* visual_input: True if using visual input to produce the 2D pixels either using the real camera or a rosbag. False if using already obtained 2D pixels.
* live_camera: True if frames are generated by an RGB-D camera (False if they are generated by rosbags)
* models_path: Set the absolute path of the models used for the prediction

NOTE: `live_camera` need to be set only if `visual_input` has been set to true.
